{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.rcParams[\"savefig.dpi\"] = 300\n",
    "plt.rcParams[\"savefig.bbox\"] = \"tight\"\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, stratify=iris.target, random_state=0)\n",
    "gbrt = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "gbrt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Illustration on synthetic regression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "def make_wave(n_samples=100):\n",
    "    rnd = np.random.RandomState(42)\n",
    "    x = rnd.uniform(-3, 3, size=n_samples)\n",
    "    y_no_noise = (np.sin(4 * x))\n",
    "    y = (y_no_noise + rnd.normal(scale=0.2, size=len(x))) / 2\n",
    "    return x.reshape(-1, 1), y\n",
    "X, y = make_wave(100)\n",
    "\n",
    "def make_poly(n_samples=100):\n",
    "    rnd = np.random.RandomState(42)\n",
    "    x = rnd.uniform(-3, 3, size=n_samples)\n",
    "    y_no_noise = (x) ** 3\n",
    "    y = (y_no_noise + rnd.normal(scale=3, size=len(x))) / 2\n",
    "    return x.reshape(-1, 1), y\n",
    "\n",
    "X, y = make_poly()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=10, learning_rate=.3, random_state=0).fit(X_train, y_train)\n",
    "gbrt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = np.linspace(X.min(), X.max(), 1000)\n",
    "preds = list(gbrt.staged_predict(line[:, np.newaxis]))\n",
    "plt.plot(X_train[:, 0], y_train, 'o', label=\"training data\")\n",
    "for i in range(1, 10, 2):\n",
    "    plt.plot(line, preds[i], label=\"n_estimators={}\".format(i + 1))\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_train = [np.zeros(len(y_train))] + list(gbrt.staged_predict(X_train))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4, 2, subplot_kw={'xticks': (()), 'yticks': (())}, figsize=(6, 6))\n",
    "#fig, axes = plt.subplots(4, 2, figsize=(12, 12))\n",
    "\n",
    "n_estimators = [0, 1, 4, 8]\n",
    "for i in range(4):\n",
    "    k = n_estimators[i]\n",
    "    axes[i, 0].plot(X_train[:, 0], y_train - preds_train[k], 'o')\n",
    "    axes[i, 0].plot(line, gbrt.estimators_[k, 0].predict(line[:, np.newaxis]))\n",
    "    axes[i, 1].plot(X_train[:, 0], y_train, 'o')\n",
    "    axes[i, 1].plot(line, preds[k])\n",
    "    axes[i, 0].set_title(\"Residual prediction step {}\".format(k + 1))\n",
    "    axes[i, 1].set_title(\"Total prediction step {}\".format(k + 1))\n",
    "\n",
    "for ax in axes.ravel():\n",
    "    ax.set_ylim(y.min(), y.max())\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/grad_boost_regression_steps.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# illustration on synthetic moon data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(noise=.2, random_state=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "gbrt = GradientBoostingClassifier(max_depth=2, n_estimators=10, learning_rate=.2, random_state=0).fit(X_train, y_train)\n",
    "gbrt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a grid for plotting decision functions...\n",
    "x_lin = np.linspace(X_train[:, 0].min() - .5, X_train[:, 0].max() + .5, 100)\n",
    "y_lin = np.linspace(X_train[:, 1].min() - .5, X_train[:, 1].max() + .5, 100)\n",
    "x_grid, y_grid = np.meshgrid(x_lin, y_lin)\n",
    "X_grid = np.c_[x_grid.ravel(), y_grid.ravel()]\n",
    "\n",
    "probs = list(gbrt.staged_predict_proba(X_grid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, subplot_kw={'xticks': (()), 'yticks': (())}, figsize=(12, 6))\n",
    "for i, (prob, ax) in enumerate(zip(probs, axes.ravel())):\n",
    "    ax.set_title(\"n_estimators={}\".format(i + 1))\n",
    "    ax.contourf(x_grid, y_grid, prob[:, 1].reshape(x_grid.shape), alpha=.4, cmap='bwr')\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', s=10)\n",
    "fig.suptitle(\"GradientBostingClassifier(max_depth=2)\")\n",
    "plt.savefig(\"images/grad_boost_depth2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {'learning_rate': [.2, .1, .05, .02, .01, .001]}\n",
    "grid = GridSearchCV(GradientBoostingClassifier(),\n",
    "                    param_grid=param_grid, cv=10,\n",
    "                    return_train_score=True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(grid.cv_results_)\n",
    "scores.plot(x='param_learning_rate', y='mean_train_score', yerr='std_train_score', ax=plt.gca())\n",
    "scores.plot(x='param_learning_rate', y='mean_test_score', yerr='std_test_score', ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston.data, boston.target, random_state=0)\n",
    "\n",
    "gbrt = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "gbrt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'learning_rate': [.5, .2, .1, .05, .02, .01, .001]}\n",
    "grid = GridSearchCV(GradientBoostingRegressor(), param_grid=param_grid, cv=10,\n",
    "                   return_train_score=True)\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "scores.plot(x='param_learning_rate', y='mean_train_score', yerr='std_train_score', ax=plt.gca())\n",
    "scores.plot(x='param_learning_rate', y='mean_test_score', yerr='std_test_score', ax=plt.gca())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbrt.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(gbrt.feature_importances_)[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    boston.data, boston.target, random_state=0)\n",
    "\n",
    "gbrt = GradientBoostingRegressor().fit(X_train, y_train)\n",
    "gbrt.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "fig, axs = plot_partial_dependence(gbrt, X_train, np.argsort(gbrt.feature_importances_)[-6:],\n",
    "                                       feature_names=boston.feature_names,\n",
    "                                       n_jobs=3, grid_resolution=50)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plot_partial_dependence(gbrt, X_train, [np.argsort(gbrt.feature_importances_)[-2:]],\n",
    "                                   feature_names=boston.feature_names,\n",
    "\n",
    "                                   n_jobs=3, grid_resolution=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, stratify=iris.target, random_state=0)\n",
    "gbrt = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "gbrt.score(X_test, y_test)\n",
    "\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "for i in range(3):\n",
    "    fig, axs = plot_partial_dependence(gbrt, X_train, range(4), n_cols=4,\n",
    "                                       feature_names=iris.feature_names, grid_resolution=50, label=i,\n",
    "                                       figsize=(8, 2))\n",
    "    fig.suptitle(iris.target_names[i])\n",
    "    for ax in axs: ax.set_xticks(())\n",
    "    for ax in axs[1:]: ax.set_ylabel(\"\")\n",
    "\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train, y_train)\n",
    "print(\"accuracy on training set: %f\" % xgb.score(X_train, y_train))\n",
    "print(\"accuracy on test set: %f\" % xgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "X_train, y_train = mnist.data[:60000], mnist.target[:60000]\n",
    "X_test, y_test = mnist.data[60000:], mnist.target[60000:]\n",
    "\n",
    "X_train, y_train = shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not a real benchmark, just illustration!\n",
    "from time import time\n",
    "times_xgb = []\n",
    "acc_xgb = []\n",
    "times_sklearn = []\n",
    "acc_sklearn = []\n",
    "n_samples_ = [10, 100, 1000, 10000]\n",
    "for n_samples in n_samples_:\n",
    "    print(n_samples)\n",
    "    # both do 100 trees, 0.1 learning rate by default\n",
    "    xgb = XGBClassifier()\n",
    "    tick = time()\n",
    "    xgb.fit(X_train[:n_samples], y_train[:n_samples])\n",
    "    times_xgb.append(time() - tick)\n",
    "    acc_xgb.append(xgb.score(X_test, y_test))\n",
    "    \n",
    "    gbrt = GradientBoostingClassifier()\n",
    "    tick = time()\n",
    "    gbrt.fit(X_train[:n_samples], y_train[:n_samples])\n",
    "    times_sklearn.append(time() - tick)\n",
    "    acc_sklearn.append(gbrt.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "times_xgb_hist = []\n",
    "acc_xgb_hist = []\n",
    "n_samples_ = [10, 100, 1000, 10000]\n",
    "for n_samples in n_samples_:\n",
    "    print(n_samples)\n",
    "    # both do 100 trees, 0.1 learning rate by default\n",
    "    xgb = XGBClassifier(tree_method=\"hist\")\n",
    "    tick = time()\n",
    "    xgb.fit(X_train[:n_samples], y_train[:n_samples])\n",
    "    times_xgb_hist.append(time() - tick)\n",
    "    acc_xgb_hist.append(xgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, = plt.plot(n_samples_, times_xgb, label=\"time xgb\")\n",
    "#tx_hist, = plt.plot(n_samples_, times_xgb_hist, label=\"time xgb hist\")\n",
    "tsk, = plt.plot(n_samples_, times_sklearn, label=\"time sklearn\")\n",
    "plt.ylabel(\"training time\")\n",
    "plt.twinx()\n",
    "ax, = plt.plot(n_samples_, acc_xgb, '--', label=\"acc xgb\")\n",
    "#ax_hist, = plt.plot(n_samples_, acc_xgb_hist, '--', label=\"acc xgb hist\")\n",
    "ask, = plt.plot(n_samples_, acc_sklearn, '--', label=\"acc sklearn\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.legend((tx, tsk, ax, ask), ('time xgb', 'time sklearn', 'acc xgb', 'acc sklearn'))\n",
    "plt.savefig(\"images/xgboost_sklearn_bench.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx, = plt.plot(n_samples_, times_xgb, label=\"time xgb\")\n",
    "tx_hist, = plt.plot(n_samples_, times_xgb_hist, label=\"time xgb hist\")\n",
    "#tsk, = plt.plot(n_samples_, times_sklearn, label=\"time sklearn\")\n",
    "plt.ylabel(\"training time\")\n",
    "plt.twinx()\n",
    "ax, = plt.plot(n_samples_, acc_xgb, '--', label=\"acc xgb\")\n",
    "ax_hist, = plt.plot(n_samples_, acc_xgb_hist, '--', label=\"acc xgb hist\")\n",
    "#ask, = plt.plot(n_samples_, acc_sklearn, '--', label=\"acc sklearn\")\n",
    "plt.ylabel(\"test accuracy\")\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.legend((tx, tx_hist, ax, ax_hist), ('time xgb', 'time xgb hist', 'acc xgb', 'acc xgb hist'))\n",
    "plt.savefig(\"images/xgboost_hist_bench.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# poor man's stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_moons(noise=.4, random_state=192, n_samples=300)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "voting = VotingClassifier([('logreg', LogisticRegression(C=100)),\n",
    "                           ('tree', DecisionTreeClassifier(max_depth=3, random_state=0)),\n",
    "                           ('knn', KNeighborsClassifier(n_neighbors=3))\n",
    "                          ],\n",
    "                         voting='soft', flatten_transform=True)\n",
    "voting.fit(X_train, y_train)\n",
    "lr, tree, knn = voting.estimators_\n",
    "print((\"{:.2f} \" * 4).format(voting.score(X_test, y_test),\n",
    "                             lr.score(X_test, y_test), tree.score(X_test, y_test),\n",
    "                             knn.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lin = np.linspace(X_train[:, 0].min() - .5, X_train[:, 0].max() + .5, 100)\n",
    "y_lin = np.linspace(X_train[:, 1].min() - .5, X_train[:, 1].max() + .5, 100)\n",
    "x_grid, y_grid = np.meshgrid(x_lin, y_lin)\n",
    "X_grid = np.c_[x_grid.ravel(), y_grid.ravel()]\n",
    "# transform produces individual probabilities\n",
    "_, lr_probs, _, tree_probs, _, knn_probs = voting.transform(X_grid).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, subplot_kw={'xticks': (()), 'yticks': (())}, figsize=(8, 2))\n",
    "titles = ['Logistic Regression', 'Decision Tree', 'KNN', 'Average']\n",
    "for prob, title, ax in zip([lr_probs, tree_probs, knn_probs, lr_probs + tree_probs + knn_probs], titles, axes.ravel()):\n",
    "    ax.set_title(title)\n",
    "    ax.contourf(x_grid, y_grid, prob.reshape(x_grid.shape), alpha=.4, cmap='bwr')\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr', s=10)\n",
    "plt.savefig(\"images/average_voting.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the negative class probabilities\n",
    "stacking = make_pipeline(voting, FunctionTransformer(lambda X: X[:, 1::2]), LogisticRegression())\n",
    "stacking.fit(X_train, y_train)\n",
    "stacking.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacking.named_steps.logisticregression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "prob = stacking.predict_proba(X_grid)\n",
    "ax.contourf(x_grid, y_grid, prob[:, 1].reshape(x_grid.shape), alpha=.4, cmap='bwr')\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "ax.set_title(\"Logistic Regression weighted\")\n",
    "plt.savefig(\"images/simple_stacking_result.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "first_stage = make_pipeline(voting, FunctionTransformer(lambda X: X[:, 1::2]))\n",
    "transform_cv = cross_val_predict(first_stage, X_train, y_train, cv=10, method=\"transform\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_stage = LogisticRegression().fit(transform_cv, y_train)\n",
    "print(second_stage.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_stage.score(transform_cv, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_stage.score(first_stage.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "prob = second_stage.predict_proba(first_stage.transform(X_grid))\n",
    "ax.contourf(x_grid, y_grid, prob[:, 1].reshape(x_grid.shape), alpha=.4, cmap='bwr')\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='bwr')\n",
    "\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "plt.title(\"Proper stacking\")\n",
    "plt.savefig(\"images/stacking_result.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "for i in range(1, 200):\n",
    "    X, y = make_moons(noise=.4, random_state=i, n_samples=300)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=0)\n",
    "\n",
    "    voting = VotingClassifier([('logreg', LogisticRegression(C=100)),\n",
    "                               ('tree', DecisionTreeClassifier(max_depth=3, random_state=0)),\n",
    "                               ('knn', KNeighborsClassifier(n_neighbors=3))\n",
    "                              ],\n",
    "                             voting='soft', flatten_transform=True)\n",
    "    voting.fit(X_train, y_train)\n",
    "    lr, tree, knn = voting.estimators_\n",
    "    scores = [voting.score(X_test, y_test),\n",
    "                                 lr.score(X_test, y_test), tree.score(X_test, y_test),\n",
    "                                 knn.score(X_test, y_test)]\n",
    "\n",
    "    # dropping the negative class probabilities\n",
    "    stacking = make_pipeline(voting, FunctionTransformer(lambda X: X[:, 1::2]), LogisticRegression())\n",
    "    stacking.fit(X_train, y_train)\n",
    "    naive_train_score = stacking.score(X_train, y_train)\n",
    "\n",
    "    naive_test_score = stacking.score(X_test, y_test)\n",
    "\n",
    "    stacking.named_steps.logisticregression.coef_\n",
    "\n",
    "    # Proper stacking\n",
    "\n",
    "    from sklearn.model_selection import cross_val_predict\n",
    "    first_stage = make_pipeline(voting, FunctionTransformer(lambda X: X[:, 1::2]))\n",
    "    transform_cv = cross_val_predict(first_stage, X_train, y_train, cv=10, method=\"transform\")\n",
    "\n",
    "    second_stage = LogisticRegression().fit(transform_cv, y_train)\n",
    "\n",
    "    train_score = second_stage.score(transform_cv, y_train)\n",
    "\n",
    "    test_score =second_stage.score(first_stage.transform(X_test), y_test)\n",
    "    if train_score > test_score and naive_train_score > naive_test_score and test_score > naive_test_score and test_score > max(scores):\n",
    "        print(i)\n",
    "    #print(train_score > test_score)\n",
    "    #print(naive_train_score > naive_test_score)\n",
    "    #print(test_score > naive_test_score)\n",
    "    #print(test_score > max(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
